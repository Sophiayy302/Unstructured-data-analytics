---
title: "Homework 1"
author: "Sophia Shi"
format:
  html:
    toc: true
    toc-location: left
    self-contained: true
jupyter: python3
---

Professional wrestling, while not everyone's cup of tea, is big business. What started as a carnival act has turned into a global entertainment industry. Netflix recently started showing Monday Night Raw, a program from the biggest North American wrestling company, WWE -- this deal is reportedly worth \$5 billion. Like any large entity, WWE is not without competition, drama, and scandal. 

## General Tips

This is very much a step-by-step process. Don't go crazy trying to get everything done with as few lines as possible. Read the documentation for the AlphaVantage api! Carefully explore the pages from cagematch. There isn't a need to get too fancy with anything here -- just go with simple function and all should be good. Don't print comments, but use normal text for explanations.

## Step 1

In the `calls` folder, you'll find 4 text files -- these are transcripts from quarterly earnings calls. Read those files in (glob.glob will be very helpful here), with appropriate column names for ticker, quarter, and year columns; this should be done within a single function. Perform any data cleaning that you find necessary. 

```{python}
import glob
import pandas as pd
import re
import os

# Get all text files in the calls folder
files = glob.glob("/Users/yuan/Desktop/Unstructured Data Analytics/HW1/calls/*.txt")

# Function to read a file and extract metadata (ticker, quarter, year)
def read_calls(file_path):
    df = pd.read_table(file_path, header=None, names=["Text"])  # Read file content
    filename = os.path.basename(file_path)  # Extract filename only

    # Extract metadata using regex
    ticker_match = re.search(r"([a-zA-Z]{2,4})_", filename)
    quarter_match = re.search(r"_q([1-4])_", filename)
    year_match = re.search(r"_(\d{4})", filename)

    # Assign extracted values or set to None if not found
    ticker = ticker_match.group(1).upper() if ticker_match else None
    quarter = f"Q{quarter_match.group(1)}" if quarter_match else None
    year = year_match.group(1) if year_match else None

    # Add extracted values to DataFrame
    df["Ticker"] = ticker
    df["Quarter"] = quarter
    df["Year"] = year

    return df

# Process all files and combine into a single DataFrame
data_frames = [read_calls(file) for file in files]
company_data = pd.concat(data_frames, ignore_index=True)

# Drop missing values (cleaning step)
company_data.dropna(inplace=True)

print(company_data)

```

## Step 2

Use the AlphaVantage api to get daily stock prices for WWE and related tickers for the last 5 years -- pay attention to your data. You cannot use any AlphaVantage packages (i.e., you can only use requests to grab the data). Tell me about the general trend that you are seeing. I don't care which viz package you use, but plotly is solid and plotnine is good for ggplot2 users.

```{python}
import requests
import pandas as pd
import json  
import matplotlib.pyplot as plt
import time  # For API request delay

API_KEY = "5qT0zvI29CudqPkIUclbkXERE3B4IdLeXufXEAzM"

# stock symbols to fetch
STOCK_SYMBOLS = ["WWE", "TKO", "EDR", "CMCSA"]

# DataFrame to store combined stock data
stock_prices = pd.DataFrame()

# Function to fetch stock data from Alpha Vantage
def fetch_stock_data(symbol, api_key):
    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize=full&apikey={api_key}'
    response = requests.get(url)

    if response.status_code == 200:
        print(f"Successfully fetched data for {symbol}")
        return response.json()  # Return JSON response
    else:
        print(f"Failed to fetch data for {symbol} (status code: {response.status_code})")
        return None

# Function to process stock data and store it in a DataFrame
def process_stock_data(symbol, raw_data):
    if raw_data and "Time Series (Daily)" in raw_data:
        print(f"Processing data for {symbol}...")
        
        # Convert daily stock data to DataFrame
        df = pd.DataFrame.from_dict(raw_data["Time Series (Daily)"], orient='index', dtype='float')
        df.index = pd.to_datetime(df.index)  # Convert index to datetime
        df.sort_index(inplace=True)  # Sort by date

        # Keep only the 'close' price
        df = df[['4. close']].rename(columns={'4. close': f'{symbol} Close'})

        # Filter for the last 5 years
        df = df[df.index >= pd.Timestamp.now() - pd.DateOffset(years=5)]
        
        return df
    else:
        print(f"Data missing or invalid for {symbol}.")
        return None

# Loop through each stock symbol and fetch/process data
for symbol in STOCK_SYMBOLS:
    print(f"\nFetching data for {symbol}...")
    raw_data = fetch_stock_data(symbol, API_KEY)
    
    # Debugging: Print first few JSON keys (optional)
    if raw_data:
        print(json.dumps(list(raw_data.keys())[:5], indent=4))  # Print first 5 keys
    
    # Process data and merge it into the main DataFrame
    stock_df = process_stock_data(symbol, raw_data)
    if stock_df is not None:
        stock_prices = stock_df if stock_prices.empty else stock_prices.join(stock_df, how='outer')
    
    time.sleep(12)  # Wait 12 seconds to avoid API rate limits

# Plot the stock prices
plt.figure(figsize=(10, 6))
for symbol in STOCK_SYMBOLS:
    col_name = f'{symbol} Close'
    if col_name in stock_prices:
        plt.plot(stock_prices.index, stock_prices[col_name], label=symbol)

# Plot the graph
plt.title("Stock Prices of WWE, TKO, EDR, CMCSA (Last 5 Years)")
plt.xlabel("Date")
plt.ylabel("Close Price (USD)")
plt.legend()
plt.grid(True)
plt.show()

```

The stock trends show strong growth for WWE until its merger with UFC under TKO, after which TKO surged in value, reflecting investor confidence. Endeavor (EDR) and Comcast (CMCSA) remained stable, with slower growth. The data suggests that the WWE-UFC merger positively impacted stock performance, positioning TKO as a rising force in the wrestling and sports entertainment market.

## Step 3

Just like every other nerdy hobby, professional wrestling draws dedicated fans. Wrestling fans often go to cagematch.net to leave reviews for matches, shows, and wrestlers. The following link contains the top 100 matches on cagematch: https://www.cagematch.net/?id=111&view=statistics

* What is the correlation between WON ratings and cagematch ratings?
```{python}
from bs4 import BeautifulSoup
import requests
import pandas as pd
import re

# Fetch the webpage
link = 'https://www.cagematch.net/?id=111&view=statistics'
top100_req = requests.get(link)
top100_soup = BeautifulSoup(top100_req.content, 'html.parser')

# Extract WON Ratings
top100_WON = top100_soup.select(".TCol.TColSeparator span.starRating")
won_list = [i.text.strip() for i in top100_WON]

# Convert WON Ratings to numeric values
def convert(i):
    stars = i.count('*')  # Count stars
    # Extract fraction if available
    fraction_match = re.search(r'(\d+)/(\d+)', i)
    if fraction_match:
        numerator, denominator = map(int, fraction_match.groups())
        fraction_value = numerator / denominator
    else:
        fraction_value = 0  # No fraction present
    return stars + fraction_value

# Convert WON ratings using the function
numeric_won = [convert(i) for i in won_list]

# Extract Cagematch Ratings
top100_rate = top100_soup.select(".TCol.TColSeparator  span.Rating.Color9")
rating_list = [float(i.text.strip()) for i in top100_rate]

# Create a DataFrame
df_matches = pd.DataFrame({
    'WON Rating': numeric_won,
    'Cagematch Rating': rating_list
})

# Calculate correlation
correlation = df_matches.corr().loc['WON Rating', 'Cagematch Rating']

# Print results
print(f"Correlation between WON Ratings and Cagematch Ratings: {correlation:.4f}")

# Display the first few rows to verify data
print(df_matches.head())
```

** Which wrestler has the most matches in the top 100?
```{python}
from bs4 import BeautifulSoup
import requests
import pandas as pd

# Step 1: Fetch the webpage
link = 'https://www.cagematch.net/?id=111&view=statistics'
response = requests.get(link)
soup = BeautifulSoup(response.content, 'html.parser')

# Step 2: Extract the match fixture data (contains wrestler names)
matches = soup.select(".TCol a")  # Selecting the match fixtures (wrestler names)
wrestler_list = []

# Step 3: Clean and split wrestler names
for match in matches:
    names = match.text.strip().split(" vs. ")  # Splitting wrestlers
    wrestler_list.extend(names)  # Add all wrestlers to the list

# Step 4: Count occurrences using Pandas
df_wrestlers = pd.DataFrame(wrestler_list, columns=["Wrestler"])
top_wrestlers = df_wrestlers["Wrestler"].value_counts()

print(top_wrestlers.head(10))

```

*** Which promotion has the most matches in the top 100? 
```{python}
from bs4 import BeautifulSoup
import requests
import pandas as pd

# Step 1: Fetch the webpage
url = "https://www.cagematch.net/?id=111&view=statistics"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Step 2: Locate the table containing match data
table = soup.find("table", class_="TBase TableBorderColor")  
rows = table.find_all("tr")[1:]  # Skip header row

# Step 3: Extract promotions
promotions = []
for row in rows:
    columns = row.find_all("td")  # Get all columns
    if len(columns) > 2:  # Ensure there is a promotion column
        promo_td = columns[2]  # The Promotion column
        promo_img = promo_td.find("img")  # Locate the image tag inside the column
        
        if promo_img and promo_img.has_attr("alt"):
            promotions.append(promo_img["alt"])  # Extract promotion name from 'alt' attribute
        else:
            promotions.append("Unknown")  # Fallback if no promotion found

# Step 4: Convert to DataFrame and count occurrences
df_promotions = pd.DataFrame({"Promotion": promotions})
promotion_counts = df_promotions["Promotion"].value_counts()

# Step 5: Find the promotion with the most matches
top_promotion = promotion_counts.idxmax()
top_count = promotion_counts.max()

# Step 6: Print results
print(f"\nThe promotion with the most matches in the top 100 is: **{top_promotion}** with {top_count} matches.\n")
print("Full list of promotions and their match counts:")
print(promotion_counts)
```

**** What is each promotion's average WON rating?
```{python}
import pandas as pd

# Assuming df_matches contains WON Rating and df_promotions contains Promotion column
# Ensure both DataFrames have the same index before merging
df_combined = df_matches.copy()
df_combined["Promotion"] = df_promotions["Promotion"]  # Add the Promotion column

# Verify the merge
print("Merged DataFrame Preview:")
print(df_combined.head())

# Ensure WON Rating is numeric
df_combined["WON Rating"] = pd.to_numeric(df_combined["WON Rating"], errors="coerce")

# Group by Promotion and calculate the average WON Rating
avg_won_by_promotion = df_combined.groupby("Promotion")["WON Rating"].mean().sort_values(ascending=False)

# Display the results
print("\nAverage WON Rating for Each Promotion:")
print(avg_won_by_promotion)
```

***** Select any single match and get the comments and ratings for that match into a data frame.
```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re

# URL of the match page with comments
URL = "https://www.cagematch.net/?id=111&nr=8034&page=99"

# Fetch and parse the webpage
response = requests.get(URL)
if response.status_code != 200:
    print(f"Failed to retrieve page (Status Code: {response.status_code})")
    exit()

soup = BeautifulSoup(response.content, "html.parser")

# Extract comments
comments_data = []
for comment in soup.find_all("div", class_="Comment"):
    author = comment.find("div", class_="CommentHeader").find("a")
    author = author.text.strip() if author else "Unknown"

    content_section = comment.find("div", class_="CommentContents")
    content = content_section.text.strip() if content_section else "No comment"

    # Extract rating if present
    rating_match = re.match(r"^\[(\d+\.\d+)\]\s*(.*)", content)
    rating = rating_match.group(1) if rating_match else "N/A"
    content = rating_match.group(2) if rating_match else content

    comments_data.append([author, rating, content])

# Convert to DataFrame and display
df_comments = pd.DataFrame(comments_data, columns=["Author", "Rating", "Comment"])
print(df_comments)

```

## Step 4

You can't have matches without wrestlers. The following link contains the top 100 wrestlers, according to cagematch: https://www.cagematch.net/?id=2&view=statistics

*** Of the top 100, who has wrestled the most matches?
***** Of the top 100, which wrestler has the best win/loss?
```{python}
import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import time

# Define headers to avoid being blocked
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}

# Base URL
BASE_URL = "https://www.cagematch.net"
WRESTLER_LIST_URL = f"{BASE_URL}/?id=2&view=statistics"

# Function to fetch the top 100 wrestlers' profile links
def get_wrestler_links():
    response = requests.get(WRESTLER_LIST_URL, headers=HEADERS)
    # Check if the request was successful
    if response.status_code != 200:
        print("Failed to fetch wrestler list.")
        return []
    # Parse the page content using BeautifulSoup
    soup = BeautifulSoup(response.text, 'html.parser')
    # Find the table containing the wrestler list
    table = soup.find("table", class_="TBase")
    if not table:
        print("Could not find the table of wrestlers.")
        return []

    wrestler_links = []
     # Loop through each row in the table
    for row in table.find_all("tr")[1:]:  # Skip header row
        # Find the first column in the row that contains the wrestler's profile link
        link_tag = row.find_all("td")[1].find("a")
        # Ensure the link is valid and contains the necessary identifier
        if link_tag and "nr=" in link_tag["href"]:
            wrestler_name = link_tag.text.strip()
            wrestler_id = re.search(r"nr=(\d+)", link_tag["href"]).group(1)
            # Construct the match statistics URL using the wrestler's ID
            match_stats_url = f"{BASE_URL}/?id=2&nr={wrestler_id}&page=22"
            wrestler_links.append((wrestler_name, match_stats_url))

    return wrestler_links

# Function to fetch match statistics for a given wrestler
def get_match_statistics(match_stats_url):
    # Send a GET request to fetch the wrestler's match statistics page
    response = requests.get(match_stats_url, headers=HEADERS)
    if response.status_code != 200:
        return None, None, None

    stats_text = BeautifulSoup(response.text, 'html.parser').get_text("\n")
    # Define a lambda function to extract numerical stats using regex
    extract_stat = lambda label: re.search(fr"{label}:\s*([\d,]+)", stats_text)
    
    # Extract match statistics
    match_count = extract_stat("Total number of matches")
    win_count = extract_stat("Total number of wins")
    loss_count = extract_stat("Total number of losses")
    
    # Convert extracted values to integers, removing commas for formatting
    return (
        int(match_count.group(1).replace(",", "")) if match_count else None,
        int(win_count.group(1).replace(",", "")) if win_count else None,
        int(loss_count.group(1).replace(",", "")) if loss_count else None
    )

# Main function to fetch stats for all wrestlers
def fetch_wrestler_stats():
    wrestler_links = get_wrestler_links() # Fetch top 100 wrestlers' profile links
    wrestler_stats = []

    for name, url in wrestler_links:
        print(f"Fetching stats for {name}...")
        matches, wins, losses = get_match_statistics(url)
        
        # Ensure all values are retrieved and valid
        if matches and wins and losses:
            wrestler_stats.append({
                'Wrestler': name,
                'Total Matches': matches,
                'Wins': wins,
                'Losses': losses,
                'Win/Loss Ratio': round(wins / losses, 2) if losses else None
            })

        time.sleep(2)  # Avoid being blocked

    return pd.DataFrame(wrestler_stats)

# Display results
df_wrestlers = fetch_wrestler_stats()
print(df_wrestlers.head())

# Identify the wrestler with the most matches
if not df_wrestlers.empty:
    top_wrestler = df_wrestlers.loc[df_wrestlers["Total Matches"].idxmax()]
    print(f"Most Matches: {top_wrestler['Wrestler']} ({top_wrestler['Total Matches']} matches)")

    best_wrestler = df_wrestlers.loc[df_wrestlers["Win/Loss Ratio"].idxmax()]
    print(f"Best Win/Loss Ratio: {best_wrestler['Wrestler']} ({best_wrestler['Win/Loss Ratio']})")

```

## Step 5

With all of this work out of the way, we can start getting down to strategy.

First, what talent should WWE pursue? Advise carefully.

WWE should pursue established wrestlers like Kazuchika Okada from New Japan Pro Wrestling, who has consistently appeared in top-rated matches based on Cagematch and WON ratings. Our data shows that New Japan Pro Wrestling leads in high-rated matches, with Okada frequently involved in some of the most critically acclaimed bouts. His proven drawing power, in-ring storytelling, and global appeal make him an ideal target for WWE to attract a wider audience and enhance match quality.


Second, reconcile what you found in steps 3 and 4 with Netflix's relationship with WWE. Use the data from the following page to help make your case: https://wrestlenomics.com/tv-ratings/

Netflix’s partnership with WWE aligns with WWE’s strong audience engagement and match quality, as shown in Steps 3 and 4. WWE consistently outperforms other wrestling brands in viewership, making it a valuable content asset for streaming. The move from cable to Netflix expands WWE’s reach, especially among younger audiences. To maximize this transition, WWE should continue recruiting top wrestlers with high match ratings, ensuring a steady flow of engaging content. Overall, the data supports WWE’s shift to Netflix as a strategic move backed by strong fan interest and match quality.

Third, do you have any further recommendations for WWE?

WWE’s move to Netflix offers global exposure. To maximize impact, WWE should create exclusive Netflix specials, documentaries, or behind-the-scenes content, leveraging the platform’s storytelling strengths to engage both casual and hardcore wrestling fans.